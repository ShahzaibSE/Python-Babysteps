{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UVSevoE8ayJY",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8Nh5RgkJayJd"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "**Due February 2, 2020 11:59**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RSAPQZTXayJe",
        "colab": {}
      },
      "source": [
        "NAME = \"\"\n",
        "STUDENT_ID = \"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h76WFLHVv0ZM",
        "colab_type": "text"
      },
      "source": [
        "## Problem 1\n",
        "\n",
        "Suppose the sample space of a random experiment is $\\{q,r,s,x,y \\}$ with probabilities 0.1, 0.2,0.3, 0.3,0.1 respectively. Let A denote the event $\\{r,s,x\\} $ and B denote the event {q,r,s}. Determine the following:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayqxZuP7v0ZQ",
        "colab_type": "text"
      },
      "source": [
        "#### a) P(A)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfTdydpEy1ml",
        "colab_type": "text"
      },
      "source": [
        "**[YOUR ANSWER HERE]**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rd-zOPnPv0ZV",
        "colab_type": "text"
      },
      "source": [
        "#### b) P(B)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1ZoZfIJAz1d_"
      },
      "source": [
        "**[YOUR ANSWER HERE]**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCMAcEWrv0ZY",
        "colab_type": "text"
      },
      "source": [
        "#### c) P($A^c$)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Dje8hca1z-jL"
      },
      "source": [
        "**[YOUR ANSWER HERE]**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGeDVYTuv0Zc",
        "colab_type": "text"
      },
      "source": [
        "#### d) P($A\\cup B$)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gFFQcC8Pz_PT"
      },
      "source": [
        "**[YOUR ANSWER HERE]**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQa9kpRHv0Zf",
        "colab_type": "text"
      },
      "source": [
        "#### e) P($A \\cap B$)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CqjJjBEEz_tI"
      },
      "source": [
        "**[YOUR ANSWER HERE]**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSGkFLcjv0Zh",
        "colab_type": "text"
      },
      "source": [
        "#### f) are the events A and B independent? Show."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SffHAtH_0G6b"
      },
      "source": [
        "**[YOUR ANSWER HERE]**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5fC0xL_v0Zj",
        "colab_type": "text"
      },
      "source": [
        "#### g) Are the events A and B mutual exclusive?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kJrXUzbj0HWa"
      },
      "source": [
        "**[YOUR ANSWER HERE]**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QloTpkwNv0Zu",
        "colab_type": "text"
      },
      "source": [
        "## Problem 2  \n",
        "A random variable w has probability density function $$f_W(w)=\\left\\{\n",
        "\\begin{array}{ll}\n",
        "      c \\dot w^2, 0\\leq w \\leq2.5 \\\\\n",
        "      1/3, 2.5 <w\\leq 5 \\\\\n",
        "      0, otherwise\\\\\n",
        "\\end{array} \n",
        "\\right.  $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64pl6AGav0Zu",
        "colab_type": "text"
      },
      "source": [
        "#### a) solve for c."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ6-4eHAv0Zx",
        "colab_type": "text"
      },
      "source": [
        "**[YOUR ANSWER HERE]**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebN6xhT9v0Zx",
        "colab_type": "text"
      },
      "source": [
        "### Problem 3\n",
        "Suppose x is a random variable with probability density function $$f_X(x) = \\frac{e^{-x}}{(1+e^{-x})^2}$$ with $-\\infty \\leq x < \\infty$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhaHLVMIv0Zy",
        "colab_type": "text"
      },
      "source": [
        "#### a) $f_X(x)$ is called the density function for the logistic distribution. Show that $f_X(x)$ is a valid probability density function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vw97STgIv0Z0",
        "colab_type": "text"
      },
      "source": [
        "**[YOUR ANSWER HERE]**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbDt4Eqcv0Z0",
        "colab_type": "text"
      },
      "source": [
        "#### b) compute the Cumulative density function, $F_X(x)$ for x."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZjVsVXDv0Z3",
        "colab_type": "text"
      },
      "source": [
        "**[YOUR ANSWER HERE]**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iq00dGzv0Z3",
        "colab_type": "text"
      },
      "source": [
        "#### c) Compute the Following:\n",
        "\n",
        "1)$F_X(30)$ \n",
        "\n",
        "2)$F_X(4)$ \n",
        "\n",
        "3)$F_X(0.5)$ \n",
        "\n",
        "4)$F_X(0)$\n",
        "\n",
        "5)$F_X(-0.5)$\n",
        "\n",
        "5)$F_X(-4)$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQfIdiLbv0Z7",
        "colab_type": "text"
      },
      "source": [
        "**[YOUR ANSWER HERE]**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAK-xVAzv0Z9",
        "colab_type": "text"
      },
      "source": [
        "####  d) Plot the shape of the curve using the results in part \"c\" The cumulative density function for the logistic distribution distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijB7_TF82Q47",
        "colab_type": "text"
      },
      "source": [
        "**[YOUR ANSWER HERE]**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0w94BYobayJh"
      },
      "source": [
        "## Problem 4\n",
        "\n",
        "Write down the factored conditional probability expression that corresponds\n",
        "to the graphical Bayesian Network shown bellow:\n",
        "\n",
        "\n",
        "![bn1](https://alliance.seas.upenn.edu/~cis520/dynamic/2018/wiki/uploads/Lectures/example_trails_bn_small.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1wV-xIShayJh"
      },
      "source": [
        "**[YOUR ANSWER HERE]**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4YAGVTJyayJi"
      },
      "source": [
        "## Problem 5\n",
        "\n",
        "Given the following Bayesian Network, find the joint probability. Write out the product of probabilities and conditional probabilites that lead to your numeric answer as well as reporting your numeric answer.\n",
        "\n",
        " ![bn2](https://drive.google.com/uc?id=1TKufu8R4QmgAY2hL3dN8BK6m6jif8L2k)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zopBGTXeayJj"
      },
      "source": [
        "#### a) $~~~$ P(Cloudy, Sprinkler, Rain, Wet_Grass) \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dgDcTIIa3Tc8"
      },
      "source": [
        "**[YOUR ANSWER HERE]**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6ovN0EmA3I3S"
      },
      "source": [
        "\n",
        "#### b) $~~~$  P(Cloudy, not Sprinkler, Rain, Wet_Grass) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6oyK5fNI3T11"
      },
      "source": [
        "**[YOUR ANSWER HERE]**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nk9KcnXW3Iap"
      },
      "source": [
        "#### c)  $~~~$   If a) < b), why?  $~~~$  OR $~~~$  If a) > b), why?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Mutsz0BG3UNv"
      },
      "source": [
        "**[YOUR ANSWER HERE]**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XvP5w8ZsayJk"
      },
      "source": [
        "### Logistic Regression ###\n",
        "\n",
        "**Refer to the following derivation of the gradient when implementing logistic regression in problem 6 below.**\n",
        "\n",
        "For this assignment we'll use the sum of squared error, and gradient descent. Suppose our dataset consists of $n$ records, each with $m$ features:\n",
        "\n",
        "$$X =\n",
        "\\begin{bmatrix}\n",
        "    x_{01}       & x_{02} & \\cdots & x_{0m} \\\\\n",
        "    x_{11}       & x_{12} & \\cdots & x_{1m} \\\\\n",
        "    \\vdots       & \\vdots & \\ddots & \\vdots \\\\\n",
        "    x_{n-1,1}       & x_{n-1,2} & \\cdots & x_{n-1,m}\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "One way to include a bias is to augment $X$ with a column of ones:\n",
        "\n",
        "$$X =\n",
        "\\begin{bmatrix}\n",
        "    1 & x_{01}       & x_{02} & \\cdots & x_{0m} \\\\\n",
        "    1 & x_{11}       & x_{12} & \\cdots & x_{1m} \\\\\n",
        "    \\vdots  & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "    1 & x_{n-1,1}       & x_{n-1,2} & \\cdots & x_{n-1,m}\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "We also have $n$ labels corresponding to the correct classification of each of the above records, $y=[y_0,y_1,\\cdots ,y_{n-1}]^T$, i.e.:\n",
        "\n",
        "$$y =\n",
        "\\begin{bmatrix}\n",
        "    y_0  \\\\\n",
        "    y_1  \\\\\n",
        "    \\vdots   \\\\\n",
        "    y_{n-1} \n",
        "\\end{bmatrix}$$\n",
        "\n",
        "We will try to find the optimal parameter values $w = [w_0, w_1, \\cdots, w_m]^T$ of our logistic regression model, where $w_0$ is the bias weight. To simplify our notation, let\n",
        " \n",
        "$$z = X w^T =\n",
        "\\begin{bmatrix}\n",
        "    X_{00}w_0 + X_{01}w_1 + \\cdots + X_{0m}w_m  \\\\\n",
        "    X_{10}w_0 + X_{11}w_1 + \\cdots + X_{1m}w_m  \\\\\n",
        "    \\vdots   \\\\\n",
        "    X_{n-1,0}w_0 + X_{n-1,1}w_1 + \\cdots + X_{n-1,m}w_m \n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "    z_0  \\\\\n",
        "    z_2  \\\\\n",
        "    \\vdots   \\\\\n",
        "    z_{n-1}\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "We seek $w$ such that the sum of squared error is minimized (the 1/2 factor makes the derivation easier):\n",
        "\n",
        "$$\\text{SSE} = \\frac{1}{2} \\sum_{i=0}^{n-1} (y_i - h_i)^2$$\n",
        "\n",
        "Since the above is a convex function, it has a unique minimum value. Taking the derivative with respect to $w_i$, we get:\n",
        "\n",
        "$$\\frac{d}{dw_j}\\text{SSE} = \\frac{1}{2} \\sum_{i=0}^{n-1} \\frac{d}{dw_j}(y_i - \\sigma(z_i))^2$$\n",
        "\n",
        "$$\\quad \\quad \\quad \\quad= \\sum_{i=0}^{n-1} (y_i - \\sigma(z_i)) \\frac{d}{dw_j} (-\\sigma(z_i))$$\n",
        "\n",
        "Recall the chain rule from calculus, and that each $z_j$ is a funcion of the $w_i$, and also that $\\frac{d}{dz}\\sigma(z)=\\sigma(z)(1-\\sigma(z))$, so the above becomes:\n",
        "\n",
        "$$\\frac{d}{dw_j}\\text{SSE} = -\\sum_{i=0}^{n-1} (y_i - \\sigma(z_i))  \\sigma(z_i) (1 -  \\sigma(z_i)) \\frac{d}{dw_j} z_i$$\n",
        "\n",
        "$$\\quad \\quad \\quad= -\\sum_{i=0}^{n-1} (y_i - \\sigma(z_i))  \\sigma(z_i) (1 -  \\sigma(z_i)) x_{ij}$$\n",
        "\n",
        "This can be written in matrix form as:\n",
        "$$\\nabla \\text{SSE} = -X^T [(y-\\sigma(z))\\sigma(z)(1-\\sigma(z))]$$\n",
        "\n",
        "The expression in the brackets is computed element-wise before taking the matrix product with $X$ transpose.\n",
        "\n",
        "We are taking a step in the opposite direction of the gradient, i.e. in the direction:\n",
        "\n",
        "$$-\\nabla \\text{SSE} = X^T [(y-\\sigma(z))\\sigma(z)(1-\\sigma(z))]$$\n",
        "\n",
        "Letting $h = \\sigma(z)$, this becomes:\n",
        "$$-\\nabla \\text{SSE} = X^T [(y-h)h(1-h)] \\quad\\quad (\\star)$$\n",
        "\n",
        "The above expression defines an $m\\times 1$ vector we use to update the $m$ weights that are the components of $w$.\n",
        "\n",
        "\n",
        "#### Note: #### \n",
        "The variables within brackets above, $[(y-h)h(1-h)]$, are column vectors (i.e. arrays of size $(n,1)$), and that the operations subtraction and multiplication are done component wise. The sigmoid function $\\sigma$ was also applied componentwise to obtain $h$ from $z$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aUx1vmBKayJl"
      },
      "source": [
        "## Problem 6\n",
        "\n",
        "Given the dataset below, implement the logistic regression algorithm, optimizing the parameters using gradient descent and squared error as the loss function. For this to go smoothly, be sure to understand what needs to be implemented in the above derivation. *Then the helper functions will almost write themselves.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Hg1zJdD1ayJm",
        "colab": {}
      },
      "source": [
        "# The following code creates the dataset we'll be using.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets.samples_generator import make_moons\n",
        "\n",
        "np.random.seed(42) \n",
        "data, labels = make_moons(n_samples=500, noise=0.1)\n",
        "colors = ['r' if y else 'b' for y in labels]\n",
        "print('data.shape =', data.shape,',  labels.shape =', labels.shape)\n",
        "plt.scatter(data[:,0], data[:,1], c=colors)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yV-5P4SRayJs"
      },
      "source": [
        "Logistic regression has a straight line for its decision boundary. Since one can't seperate the blue and red dots with a line, a logistic regression model can't possibly perfectly distinguish blue from red dots in this dataset. Our model can't possibly achieve 100% accuracy on this data.\n",
        "\n",
        "#### Note: ####\n",
        "For your code below, note that\n",
        "$$h = \\sigma(z)$$\n",
        "and\n",
        "$$-\\nabla \\text{SSE} = X^T [(y-h)h(1-h)] \\quad\\quad (\\star)$$\n",
        "\n",
        "That last expression is what you're to implement for the negative of the gradient.                       \n",
        "\n",
        "Also notice that $X^T$ is the transpose of matrix $X$, and that we have a matrix multiplication going on between the matrix and the column vector inside the brackets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NslXYOeuayJt",
        "colab": {}
      },
      "source": [
        "# Complete the missing code.  Feel free to create helper functions as needed.\n",
        "\n",
        "\n",
        "def logistic_regression(x, y, learning_rate, num_steps=40):\n",
        "    ''' Input:  x = the data\n",
        "                y = the labels\n",
        "                learning_rate = learning rate\n",
        "                num_steps = number of iterations \n",
        "        Output: w = the trained model weights    '''\n",
        "    \n",
        "    # Start by intializing the weights w with w_i = 1 for all i, and make it a 3x1 \n",
        "    # column vector (numpy array).  You can use the numpy function, ones.\n",
        "    # YOUR CODE HERE\n",
        "    w = np.ones(len(y)) # MODIFY THIS LINE!\n",
        "    \n",
        "    # Augment x with an initial column of ones for the bias term (the zeroth column of x).\n",
        "    # You can use the numpy functions ones and hstack to accomplish this.\n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    # Set z equal to the dot product of x and w.\n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    # Set h equal to the sigmoid function of z.\n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    # Compute the initial accuracy, by finding the proportion of times your model's prediction \n",
        "    # agrees with the labeled values y. Note that you'll have to use the numpy round function\n",
        "    # to turn your h values into actual predictions.\n",
        "    accuracy = None  # YOUR CODE HERE \n",
        "    print('Intial Accuracy: ', accuracy)\n",
        "    \n",
        "    for step in range(num_steps):\n",
        "        \n",
        "        # Set z equal to the dot product of x and W.\n",
        "        # YOUR CODE HERE\n",
        "    \n",
        "        # Set h equal to the sigmoid function of z.\n",
        "        # YOUR CODE HERE\n",
        "        \n",
        "        # Calculate the negative of the gradient.\n",
        "        neg_gradient = None   # YOUR CODE HERE\n",
        "        \n",
        "        # Update weights.\n",
        "        # Increase each weight by the corresponding learning_rate * neg_gradient.\n",
        "        # YOUR CODE HERE\n",
        "        \n",
        "        # Compute the accuracy for this iteration, by finding the proportion of times your \n",
        "        # model's prediction agrees with the label values y. Note that you'll have to use the \n",
        "        # numpy round function to turn your h values into actual predictions.\n",
        "        accuracy = None  # YOUR CODE HERE \n",
        "        print('Step', step + 1, ' Accuracy: ', accuracy)\n",
        "        \n",
        "    return w\n",
        "\n",
        "\n",
        "ws = logistic_regression(data, labels.reshape((len(labels), 1)), 0.5)\n",
        "\n",
        "# Set z equal to the dot product of x and W.\n",
        "z = None # YOUR CODE HERE\n",
        "\n",
        "# Set h equal to the sigmoid function of z.\n",
        "h = None  # YOUR CODE HERE\n",
        "\n",
        "# Set y equal to your model's predictions.\n",
        "# YOUR CODE HERE\n",
        "y = np.ones((len(labels), 1))   # YOUR CODE HERE\n",
        "\n",
        "# Plot the correct classifications in green, and the classification errors in red.\n",
        "colors = ['g' if h==y else 'r' for h, y in zip(y, labels.astype(np.int))]\n",
        "plt.title('Classification Results')\n",
        "plt.scatter(data[:,0], data[:,1], c=colors)\n",
        "\n",
        "coeffs = (ws.T).tolist()[0]   # the learned logistic regression coefficients\n",
        "xvals = [-1.2, 2.2]\n",
        "\n",
        "# Weirdly, plotting the decision boundary, it's vertically off by about 0.135.  This seems to be a bug\n",
        "# in matplotlib, since it's off by this amount even changing the seed used to generate the data.\n",
        "#yvals = [-coeffs[1]/coeffs[2] * xval - coeffs[0]/coeffs[2] - 0.135 for xval  in xvals]\n",
        "#plt.plot([xvals[0], xvals[1]], [yvals[0], yvals[1]], 'b-')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}